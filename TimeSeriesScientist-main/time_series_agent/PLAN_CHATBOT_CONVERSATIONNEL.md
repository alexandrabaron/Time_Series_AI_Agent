# ü§ñ Plan d'Impl√©mentation : Chatbot Conversationnel avec Llama3 (Ollama)

## üìã Objectif

Transformer l'orchestrateur actuel en un **vrai chatbot conversationnel intelligent** qui peut :
- R√©pondre aux questions de l'utilisateur sur les donn√©es, analyses, et r√©sultats
- Expliquer les d√©cisions prises par les agents
- Guider l'utilisateur √† travers le workflow
- Utiliser **Llama3 en local via Ollama** (pas d'API externe)

---

## üîç √âtat Actuel

### Ce qui fonctionne d√©j√† ‚úÖ
1. **Interface de chat Streamlit** : affichage des messages, input utilisateur
2. **Gestion de session** : `SessionManager` maintient l'√©tat et l'historique
3. **Orchestrateur basique** : `ConversationalOrchestrator` g√®re les commandes pr√©d√©finies
4. **Agents wrappers** : `PreprocessAgentWrapper`, `AnalysisAgentWrapper` retournent des r√©sultats structur√©s

### Ce qui ne fonctionne PAS ‚ùå
1. **Pas de vraie conversation** : L'input utilisateur fait juste un echo (ligne 358 de `streamlit_app.py`)
2. **`answer_question()` est hardcod√©e** : R√©ponses if/else limit√©es (lignes 300-324 de `conversational_orchestrator.py`)
3. **Pas d'int√©gration LLM local** : Ollama/Llama3 n'est pas configur√©
4. **Pas de contexte dynamique** : Le LLM n'a pas acc√®s aux r√©sultats d'analyse en session

---

## üõ†Ô∏è Plan d'Impl√©mentation

### **√âtape 1 : Installation et Configuration d'Ollama** üîß

#### 1.1 Pr√©requis
- V√©rifier qu'Ollama est install√© sur votre machine
- V√©rifier que le mod√®le Llama3 est t√©l√©charg√© : `ollama pull llama3`
- Tester qu'Ollama fonctionne : `ollama run llama3`

#### 1.2 Ajouter les d√©pendances
Modifier `requirements.txt` pour ajouter :
```txt
langchain-community>=0.0.10
```

**Note** : `langchain-community` contient l'int√©gration Ollama.

---

### **√âtape 2 : Cr√©er un Module LLM Local** üß†

#### 2.1 Cr√©er `utils/local_llm.py`
Un module d√©di√© pour g√©rer l'interaction avec Ollama/Llama3.

**Fonctionnalit√©s** :
- Initialiser la connexion Ollama
- G√©rer les prompts et r√©ponses
- Retry logic en cas d'erreur
- Fallback vers r√©ponses hardcod√©es si Ollama est down

**API** :
```python
class LocalLLM:
    def __init__(self, model="llama3", base_url="http://localhost:11434"):
        """Initialize Ollama LLM."""
        
    def ask(self, prompt: str, context: Dict[str, Any] = None) -> str:
        """Send a question to the local LLM with context."""
        
    def is_available(self) -> bool:
        """Check if Ollama is running."""
```

---

### **√âtape 3 : Cr√©er un Syst√®me de Contexte Conversationnel** üìö

#### 3.1 Cr√©er `utils/conversation_context.py`
Un module pour construire le contexte dynamique √† envoyer au LLM.

**Fonctionnalit√©s** :
- Extraire les informations pertinentes de `st.session_state`
- Formater le contexte en texte lisible pour le LLM
- Inclure : dataset info, r√©sultats d'analyse, recommandations, √©tape actuelle

**API** :
```python
class ConversationContextBuilder:
    @staticmethod
    def build_context(session_state) -> str:
        """Build a comprehensive context string from session state."""
        # Inclut :
        # - Dataset info (colonnes, taille, qualit√©)
        # - R√©sultats de preprocessing (valeurs manquantes, outliers)
        # - R√©sultats d'analyse (tendance, saisonnalit√©, stationnarit√©)
        # - Recommandations de mod√®les
        # - √âtape actuelle du workflow
```

---

### **√âtape 4 : Am√©liorer l'Orchestrateur Conversationnel** üéØ

#### 4.1 Modifier `graph/conversational_orchestrator.py`

**Changements** :
1. Int√©grer `LocalLLM` dans `__init__()`
2. Remplacer `answer_question()` hardcod√©e par un appel au LLM avec contexte
3. Ajouter un syst√®me de routing intelligent :
   - Si la question est une commande (ex: "lance l'analyse") ‚Üí router vers `handle_command()`
   - Si c'est une question (ex: "pourquoi ARIMA ?") ‚Üí router vers `answer_question()` avec LLM

**Nouveau workflow** :
```python
def handle_user_input(self, user_input: str) -> Dict[str, Any]:
    """
    Main entry point for user input.
    Routes to command handler or question answering.
    """
    # 1. Intent detection (commande vs question)
    intent = self._detect_intent(user_input)
    
    # 2. Route accordingly
    if intent == 'command':
        return self.handle_command(extracted_command, user_input)
    elif intent == 'question':
        return self.answer_question(user_input)
    else:
        return self._fallback_response(user_input)
```

#### 4.2 Am√©liorer `answer_question()`
```python
def answer_question(self, question: str) -> Dict[str, Any]:
    """Answer user question using local LLM with context."""
    # 1. Build context from session state
    context = ConversationContextBuilder.build_context(st.session_state)
    
    # 2. Create prompt for LLM
    prompt = f"""Tu es TSci-Chat, un assistant expert en analyse de s√©ries temporelles.
    
CONTEXTE ACTUEL :
{context}

QUESTION DE L'UTILISATEUR :
{question}

R√©ponds de mani√®re claire, concise et pr√©cise en te basant sur le contexte fourni.
Si la r√©ponse n'est pas dans le contexte, dis-le honn√™tement.
"""
    
    # 3. Get answer from LLM
    answer = self.local_llm.ask(prompt)
    
    # 4. Return formatted response
    return {
        'status': 'success',
        'message': answer,
        'is_question': True
    }
```

---

### **√âtape 5 : Int√©grer dans l'UI Streamlit** üñ•Ô∏è

#### 5.1 Modifier `ui/streamlit_app.py`

**Remplacer** la ligne 358 (echo hardcod√©) par :
```python
if user_input:
    # Route to orchestrator
    result = st.session_state.orchestrator.handle_user_input(user_input)
    
    # Add response to chat
    add_assistant_message(result['message'])
    st.rerun()
```

---

### **√âtape 6 : Am√©liorer les Prompts pour les Agents** üìù

#### 6.1 Objectif
Permettre au chatbot d'expliquer les d√©cisions des agents (ex: "Pourquoi ARIMA ?", "Pourquoi interpoler ?")

#### 6.2 Approche
- Les wrappers d'agents retournent d√©j√† des `reasons` dans leurs r√©sultats
- L'orchestrateur doit stocker ces justifications dans `st.session_state.results`
- Le contexte conversationnel inclut ces justifications
- Le LLM peut alors les expliquer de mani√®re conversationnelle

**Exemple** :
```
User: "Pourquoi as-tu choisi l'interpolation ?"

Context: {
  "preprocessing": {
    "missing_strategy": "interpolate",
    "reason": "Bon pour pr√©server les tendances dans les donn√©es continues"
  }
}

LLM Response: "J'ai choisi l'interpolation pour les valeurs manquantes car 
elle est particuli√®rement adapt√©e aux s√©ries temporelles continues comme la 
v√¥tre. Elle pr√©serve les tendances naturelles en estimant les valeurs 
manquantes √† partir des points voisins."
```

---

## üß™ Plan de Test

### Test 1 : Ollama fonctionne
```bash
ollama list  # V√©rifier que llama3 est install√©
ollama run llama3 "Bonjour, es-tu pr√™t ?"
```

### Test 2 : LocalLLM module
```python
from utils.local_llm import LocalLLM
llm = LocalLLM()
assert llm.is_available()
response = llm.ask("Qu'est-ce qu'ARIMA ?")
print(response)
```

### Test 3 : Contexte conversationnel
```python
from utils.conversation_context import ConversationContextBuilder
context = ConversationContextBuilder.build_context(st.session_state)
print(context)  # Doit afficher dataset info, r√©sultats, etc.
```

### Test 4 : Questions conversationnelles (E2E)
1. Uploader un dataset
2. Lancer le preprocessing
3. Poser des questions :
   - "Quelle est la qualit√© de mes donn√©es ?"
   - "Combien de valeurs manquantes ?"
   - "Pourquoi utiliser l'interpolation ?"
   - "Mes donn√©es ont-elles une tendance ?"
   - "Quels mod√®les recommandes-tu ?"

---

## ‚ö†Ô∏è Risques et Solutions

### Risque 1 : Ollama n'est pas install√© ou ne fonctionne pas
**Solution** : Fallback vers r√©ponses hardcod√©es + message d'erreur clair

### Risque 2 : Llama3 est lent sur l'ordinateur de l'utilisateur
**Solution** : 
- Afficher un spinner pendant le traitement
- Utiliser un mod√®le plus petit (ex: `llama3:8b` au lieu de `llama3:70b`)
- Limiter la longueur du contexte envoy√© au LLM

### Risque 3 : Le LLM donne des r√©ponses incorrectes ou hallucine
**Solution** :
- Prompts tr√®s structur√©s avec instruction claire : "Base-toi UNIQUEMENT sur le contexte fourni"
- Validation des r√©ponses critiques avant affichage
- Permettre √† l'utilisateur de signaler des r√©ponses incorrectes

### Risque 4 : L'intent detection rate
**Solution** :
- Utiliser aussi Llama3 pour l'intent detection
- Avoir des mots-cl√©s de secours (ex: "lance", "montre", "affiche" ‚Üí commande)

---

## üì¶ R√©sum√© des Fichiers √† Cr√©er/Modifier

### Nouveaux fichiers
1. `utils/local_llm.py` - Int√©gration Ollama/Llama3
2. `utils/conversation_context.py` - Construction du contexte conversationnel
3. `PLAN_CHATBOT_CONVERSATIONNEL.md` - Ce document

### Fichiers √† modifier
1. `requirements.txt` - Ajouter `langchain-community`
2. `graph/conversational_orchestrator.py` - Ajouter `handle_user_input()`, am√©liorer `answer_question()`
3. `ui/streamlit_app.py` - Connecter l'input utilisateur √† l'orchestrateur (ligne 358)
4. `utils/session_manager.py` - Potentiellement ajouter des m√©thodes pour stocker les justifications

---

## üöÄ Ordre d'Impl√©mentation Recommand√©

1. **V√©rifier Ollama** : S'assurer qu'Ollama + Llama3 fonctionnent
2. **Cr√©er `local_llm.py`** : Module de base pour communiquer avec Ollama
3. **Tester LocalLLM** : V√©rifier que les questions/r√©ponses fonctionnent
4. **Cr√©er `conversation_context.py`** : Builder de contexte
5. **Modifier `conversational_orchestrator.py`** : Int√©grer LocalLLM et am√©liorer `answer_question()`
6. **Modifier `streamlit_app.py`** : Connecter l'input √† l'orchestrateur
7. **Tests E2E** : Tester des conversations r√©elles

---

## üí° Am√©liorations Futures (Optionnel)

1. **M√©moire conversationnelle** : Inclure les 5 derniers messages dans le contexte pour des conversations multi-tours
2. **RAG (Retrieval Augmented Generation)** : Stocker les r√©sultats dans une base vectorielle pour r√©cup√©ration intelligente
3. **Multi-modal** : Permettre au LLM de "voir" les graphiques et les commenter
4. **Fine-tuning** : Fine-tuner Llama3 sur des conversations sp√©cifiques aux s√©ries temporelles

---

## ‚úÖ Checklist de Validation

- [ ] Ollama est install√© et fonctionne
- [ ] Llama3 est t√©l√©charg√© localement
- [ ] `LocalLLM` peut communiquer avec Ollama
- [ ] Le contexte conversationnel est construit correctement
- [ ] L'orchestrateur route correctement les questions vs commandes
- [ ] L'UI envoie l'input utilisateur √† l'orchestrateur
- [ ] Le chatbot r√©pond de mani√®re pertinente aux questions
- [ ] Le chatbot peut expliquer les d√©cisions des agents
- [ ] Le chatbot fonctionne √† toutes les √©tapes du workflow

---

**Pr√™t √† impl√©menter ?** üöÄ

