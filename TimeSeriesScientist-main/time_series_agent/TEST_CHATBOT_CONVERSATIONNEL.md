# üß™ Test Guide : Chatbot Conversationnel avec Llama3

## ‚úÖ Pr√©requis

Avant de tester, v√©rifiez que :
1. **Ollama est install√© et en cours d'ex√©cution**
2. **Llama3 est t√©l√©charg√©**

### V√©rification Ollama

```bash
# V√©rifier qu'Ollama fonctionne
ollama list

# Vous devriez voir :
# NAME         ID           SIZE    MODIFIED
# llama3:...   ...          4.7GB   ...

# Si llama3 n'est pas install√© :
ollama pull llama3

# D√©marrer Ollama (si n√©cessaire)
ollama serve
```

---

## üì¶ Installation des D√©pendances

```bash
cd TimeSeriesScientist-main/time_series_agent
pip install langchain-community
```

---

## üöÄ Lancement de l'Application

```bash
python main.py
```

L'application devrait s'ouvrir dans votre navigateur √† `http://localhost:8501`.

---

## üß™ Tests √† Effectuer

### **Test 1 : V√©rification que le LLM est disponible**

**Objectif** : S'assurer qu'Ollama est correctement connect√©.

**Proc√©dure** :
1. Ouvrez l'application
2. Dans le chat, tapez : `Bonjour, es-tu l√† ?`
3. **R√©sultat attendu** : Le bot r√©pond de mani√®re conversationnelle (ex: "Bonjour ! Oui, je suis l√† pour vous aider...")

**Si √ßa ne fonctionne pas** :
- V√©rifiez qu'Ollama est en cours d'ex√©cution : `ollama serve`
- V√©rifiez les logs dans le terminal pour voir les erreurs

---

### **Test 2 : Questions sur le Dataset (sans donn√©es charg√©es)**

**Objectif** : Tester les r√©ponses quand aucune donn√©e n'est disponible.

**Proc√©dure** :
1. Sans uploader de fichier, posez les questions suivantes :
   - `Quelle est la qualit√© de mes donn√©es ?`
   - `Combien de valeurs manquantes ?`
   - `Mes donn√©es ont-elles une tendance ?`

**R√©sultat attendu** :
- Le bot r√©pond qu'aucune donn√©e n'est disponible et invite √† uploader un fichier

---

### **Test 3 : Workflow Complet avec Questions**

**Objectif** : Tester le chatbot tout au long du workflow.

#### √âtape 1 : Upload du Dataset
1. Uploadez `ETTh1.csv` (ou votre fichier)
2. S√©lectionnez `date` et `OT` (ou votre colonne cible)

#### √âtape 2 : Questions Pr√©liminaires
Posez ces questions :
- `Combien de lignes dans mon dataset ?`
- `Quelle est la colonne cible ?`
- `R√©sume-moi les informations du dataset`

**R√©sultat attendu** :
- Le bot r√©pond avec les informations exactes du dataset

#### √âtape 3 : Pr√©traitement
1. Cliquez sur **"üöÄ 1. Lancer le Pr√©-traitement"**
2. Attendez l'analyse
3. Appliquez les strat√©gies recommand√©es

#### √âtape 4 : Questions sur le Pr√©traitement
Posez ces questions :
- `Pourquoi utiliser l'interpolation pour les valeurs manquantes ?`
- `Combien d'outliers ont √©t√© d√©tect√©s ?`
- `Quelle est la qualit√© de mes donn√©es ?`

**R√©sultat attendu** :
- Le bot explique les choix de preprocessing en se basant sur le contexte
- Il cite les chiffres exacts (nombre d'outliers, pourcentage, etc.)

#### √âtape 5 : Analyse Statistique
1. Cliquez sur **"üìä 2. Lancer l'Analyse"**
2. S√©lectionnez la p√©riode saisonni√®re (ex: 168 pour hebdomadaire)
3. Attendez les r√©sultats

#### √âtape 6 : Questions sur l'Analyse
Posez ces questions :
- `Mes donn√©es ont-elles une tendance ?`
- `Y a-t-il de la saisonnalit√© ?`
- `Les donn√©es sont-elles stationnaires ?`
- `Quels mod√®les recommandes-tu ?`
- `Pourquoi ARIMA ?`

**R√©sultat attendu** :
- Le bot r√©pond avec les r√©sultats de l'analyse
- Il explique les recommandations de mod√®les
- Il justifie pourquoi certains mod√®les sont adapt√©s

---

### **Test 4 : Commandes en Langage Naturel**

**Objectif** : Tester la d√©tection d'intent et l'extraction de commandes.

**Proc√©dure** :
Essayez de donner des commandes en langage naturel :
- `Lance l'analyse statistique`
- `Peux-tu analyser mes donn√©es ?`
- `Montre-moi les pr√©visions` (si impl√©ment√©)
- `G√©n√®re un rapport`

**R√©sultat attendu** :
- Le bot d√©tecte qu'il s'agit d'une commande
- Il ex√©cute la commande appropri√©e (ex: lance l'analyse)
- Ou indique que la fonctionnalit√© n'est pas encore impl√©ment√©e

---

### **Test 5 : Questions Multi-Tours**

**Objectif** : Tester que le bot peut r√©pondre √† plusieurs questions successives.

**Proc√©dure** :
Posez une s√©rie de questions :
1. `Quelle est la tendance ?`
2. `Et la saisonnalit√© ?`
3. `Pourquoi cette p√©riode ?`
4. `Quels mod√®les pour ce type de donn√©es ?`

**R√©sultat attendu** :
- Le bot r√©pond √† chaque question en se basant sur le contexte actuel
- Les r√©ponses sont coh√©rentes et li√©es

---

### **Test 6 : Fallback quand Ollama est Down**

**Objectif** : Tester le comportement quand Ollama n'est pas disponible.

**Proc√©dure** :
1. **Arr√™tez Ollama** : Fermez le processus Ollama
2. Rechargez l'application Streamlit
3. Posez une question : `Quelle est la qualit√© de mes donn√©es ?`

**R√©sultat attendu** :
- Le bot indique qu'Ollama n'est pas disponible
- Il fournit des r√©ponses fallback basiques (hardcod√©es) pour les questions simples
- Il sugg√®re de red√©marrer Ollama

---

### **Test 7 : Questions Hors Contexte**

**Objectif** : Tester que le bot ne "hallucine" pas et reste dans le contexte.

**Proc√©dure** :
Posez des questions sans rapport avec vos donn√©es :
- `Quelle est la capitale de la France ?`
- `Comment faire un g√¢teau ?`
- `Explique-moi la m√©canique quantique`

**R√©sultat attendu** :
- Le bot indique qu'il ne peut r√©pondre qu'√† des questions li√©es √† l'analyse de s√©ries temporelles
- Ou redirige vers des questions pertinentes

---

## üêõ Probl√®mes Fr√©quents et Solutions

### Probl√®me 1 : `ModuleNotFoundError: No module named 'langchain_community'`

**Solution** :
```bash
pip install langchain-community
```

### Probl√®me 2 : "Ollama n'est pas disponible"

**Solution** :
```bash
# D√©marrez Ollama
ollama serve

# Dans un autre terminal, testez
ollama run llama3 "Hello"
```

### Probl√®me 3 : Le bot est tr√®s lent

**Cause** : Llama3 peut √™tre lent sur certains ordinateurs

**Solutions** :
- Utilisez un mod√®le plus petit : `llama3:8b` au lieu de `llama3:70b`
- Dans `utils/local_llm.py`, modifiez :
  ```python
  self._llm = Ollama(
      model="llama3:8b",  # Plus rapide
      ...
  )
  ```

### Probl√®me 4 : R√©ponses incoh√©rentes

**Cause** : Le contexte n'est pas bien construit ou le prompt n'est pas assez pr√©cis

**Solution** :
- V√©rifiez que les r√©sultats sont bien stock√©s dans `st.session_state.results`
- Ajoutez des logs pour voir le contexte envoy√© au LLM :
  ```python
  logger.info(f"Context: {context}")
  ```

---

## üìä Crit√®res de Succ√®s

L'impl√©mentation est consid√©r√©e comme r√©ussie si :

- [ ] Ollama se connecte correctement
- [ ] Le bot r√©pond aux questions simples (qualit√©, valeurs manquantes, outliers)
- [ ] Le bot r√©pond aux questions d'analyse (tendance, saisonnalit√©, stationnarit√©)
- [ ] Le bot explique les recommandations de mod√®les
- [ ] Les commandes en langage naturel sont d√©tect√©es et ex√©cut√©es
- [ ] Le fallback fonctionne quand Ollama est down
- [ ] Le bot reste dans le contexte (pas d'hallucinations)

---

## üìù Rapport de Test

Apr√®s avoir effectu√© les tests, remplissez ce rapport :

### ‚úÖ Tests R√©ussis
- Test 1 : ‚òê
- Test 2 : ‚òê
- Test 3 : ‚òê
- Test 4 : ‚òê
- Test 5 : ‚òê
- Test 6 : ‚òê
- Test 7 : ‚òê

### ‚ùå Tests √âchou√©s
- (Listez les tests qui ont √©chou√© et les erreurs observ√©es)

### üêõ Bugs Identifi√©s
- (Listez les bugs rencontr√©s)

### üí° Am√©liorations Sugg√©r√©es
- (Suggestions pour am√©liorer le chatbot)

---

## üöÄ Prochaines √âtapes

Si tous les tests passent, vous pouvez :
1. **Am√©liorer les prompts** : Affiner les instructions donn√©es au LLM
2. **Ajouter la m√©moire conversationnelle** : Inclure les derniers messages dans le contexte
3. **Int√©grer les agents restants** : ValidationAgent, ForecastAgent, ReportAgent
4. **Optimiser les performances** : R√©duire le temps de r√©ponse

---

**Bon test ! üéØ**

